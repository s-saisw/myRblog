---
title: "Heckit"
---

We want to estimate this model
$$y = \mathbf{x\beta}+u,\ E(u|\mathbf{x})=0$$

However, $y$ is not observed for some observations. 

Suppose the selection process follows this equation.
$$s = \mathcal{1}[\mathbf{z\gamma} + v \ge 0],$$
where $s=1$ if $y$ is observed, 0 otherwise.

We can show that

$$E(y|\mathbf{z},s=1) = \mathbf{x}\beta + \rho\lambda(\mathbf{z\gamma}),$$
where $\lambda(\mathbf{z\gamma})$ is the inverse Mills ratio.

This equation means we can estimate $\beta$ using only the selected sample if we include $\lambda(\mathbf{z\gamma})$ as an additional regressor. This means we can correct for sample selection in two steps:

1. Using all observations, estimate a probit model of $s_i$ on $\mathbf{z}_i$ and obtain the estimates $\mathbf{\hat{\gamma}}$. Compute the inverse Mills ratio $\hat{\lambda}_i = \lambda(\mathbf{z_i\hat{\gamma}}) = \phi(\mathbf{z_i\hat{\gamma}})/\Phi(\mathbf{z_i\hat{\gamma}})$ for each $i$. 

2. Using the selected sample, i.e. the observations for which $s_i = 1$, run the regression of $y_i$ on $\mathbf{x}_i$ and $\hat{\lambda}_i$. Obtain $\mathbf{\hat{\beta}}$.

## Heckit by Hand: Computer Exercise 17.7

1. Use the *mroz* data for this exercise. Using the 428 women who were in the workforce, regress $\log(wage)$ on $educ$ using OLS. Include $exper$, $exper^2$, $nwifeinc$, $age$, $kidslt6$, and $kidsge6$ as explanatory variables. Report your estimate on educ and its standard error.

2. Now, estimate the return to education by Heckit, where all exogenous variables show up in the second-stage regression. In other words, the regression is $\log (wage)$ on $educ$, $exper$, $exper^2$, $nwifeinc$, $age$, $kidslt6$, $kidsge6$ and $\hat{\lambda}$. Compare the estimated return to education and its standard error to that from Question 1. (Hint: We can access the linear fit using *[glmobject]$linear.predictors*.)

3. Using only the 428 observations for working women, regress $\hat{\lambda}$ on $educ$, $exper$, $exper^2$, $nwifeinc$, $age$, $kidslt6$, and $kidsge6$. How big is the $R^2$? How does this help explain your findings from Question 2?

### Question 1

```{r, warning = FALSE, message=FALSE}
library(wooldridge)
library(tidyverse)
q1 <- lm(data = mroz, 
         lwage ~ 1 + educ + 
           exper + expersq + nwifeinc + age + kidslt6 + kidsge6)
coefficients(summary(q1))[,1:3]
```

### Question 2

```{r}
# Step 1
mroz$s <- !is.na(mroz$lwage) %>% as.numeric()
step1 <- glm(data = mroz,
             s ~ educ + exper + expersq + nwifeinc + age + kidslt6 + kidsge6,
             family = binomial(link = "probit"))
mroz$imr <- dnorm(step1$linear.predictors) / pnorm(step1$linear.predictors)
# Step 2
step2 <- lm(data = mroz[mroz$s==1,],
            lwage ~ 1 + educ + 
           exper + expersq + nwifeinc + age + kidslt6 + kidsge6 + imr)
```

```{r}
summary(step2)
```

After sample selection correction, the return to education becomes larger but standard error is twice as large.

### Question 3

```{r}
q3 <- lm(data = mroz[mroz$s==1,],
         imr ~ 1 + educ + exper + expersq + nwifeinc + age + kidslt6+kidsge6)
summary(q3)$r.squared
```

There is substantial multicollinearity in the second stage regression. Thus, large standard errors. 

## Automatic Heckit

Instead of doing Heckit by hand, we can also use the *heckit()* function from *sampleSelection* package.

```{r, warning=FALSE, message = FALSE}
library(sampleSelection)
mroz.heckit <- mroz %>% 
  heckit(selection = s ~ educ + exper + expersq + nwifeinc +
           age + kidslt6 + kidsge6,
         outcome = lwage ~ educ + exper + expersq + nwifeinc +
           age + kidslt6 + kidsge6)
summary(mroz.heckit)
```

The estimates are the same as the ones we obtained from doing heckit by hand.

## Issues about Inverse Mill's Ratio
There are several definitions of Inverse Mill's ratio. For example, the ones listed on [this forum](https://www.stata.com/support/faqs/statistics/inverse-mills-ratio/). However, in the context of Heckit, we need to use the following definition:

$$\hat{\lambda}_i = \lambda(\mathbf{z_i\hat{\gamma}}) = \phi(\mathbf{z_i\hat{\gamma}})/\Phi(\mathbf{z_i\hat{\gamma}})$$ 

Proof: Suppose our original model is $y = X\beta + u$ and selection process is $s = \mathcal{1}[\mathbf{z\gamma} + v \ge 0]$. Then,

$$
\begin{aligned}
 E(y_i | s_i = 1, z_i) &=  E[X_i \beta + u_i|s_i = 1, z_i]\\
 &= E[X_i \beta + u_i| v_i \ge -\mathbf{z_i\gamma}] \\
 &= X_i \beta + E[u_i| v_i \ge -\mathbf{z_i\gamma}]
\end{aligned}
$$

Let $\epsilon_i$ be the residual from regressing $u$ on $v$. Then $\epsilon_i = u_i - \rho v_i$ and $E(\epsilon)= 0$ and $Cov(v_i,\epsilon_i)=0$.

Therefore, 

$$
\begin{aligned}
 E[u_i| v_i \ge -\mathbf{z_i\gamma}] &= 
 E[\epsilon_i +\rho v_i| v_i \ge -\mathbf{z_i\gamma}] \\
 &= E[\epsilon_i | v_i \ge -\mathbf{z_i\gamma}] +
 E[\rho v_i| v_i \ge -\mathbf{z_i\gamma}]\\
&= 0 + \rho E[v_i| v_i \ge -\mathbf{z_i\gamma}] \\
&= \rho E[v_i| v_i \ge -\mathbf{z_i\gamma}] \\
&=\rho\frac{\phi(\mathbf{-z_i\gamma})}{1-\Phi(\mathbf{-z_i\gamma})} \\
&=\rho\frac{\phi(\mathbf{z_i\gamma})}{\Phi(\mathbf{z_i\gamma})}
\end{aligned} 
$$